hostname sb015.cluster
ipconfig 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc mq state UP group default qlen 1000 link/ether a0:8c:f8:61:07:ba brd ff:ff:ff:ff:ff:ff inet 10.3.81.95/22 brd 10.3.83.255 scope global enp4s0f0 valid_lft forever preferred_lft forever inet6 fe80::a28c:f8ff:fe61:7ba/64 scope link valid_lft forever preferred_lft forever 3: enp4s0f1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:61:07:bb brd ff:ff:ff:ff:ff:ff 4: enp4s0f2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:61:07:bc brd ff:ff:ff:ff:ff:ff 5: enp4s0f3: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:61:07:bd brd ff:ff:ff:ff:ff:ff 6: ib0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 4092 qdisc pfifo_fast state UP group default qlen 256 link/infiniband 80:00:00:29:fe:80:00:00:00:00:00:00:88:66:39:03:00:23:bc:d1 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff inet 10.3.89.95/22 brd 10.3.91.255 scope global ib0 valid_lft forever preferred_lft forever inet 10.3.93.95/22 brd 10.3.95.255 scope global ib0:1 valid_lft forever preferred_lft forever inet6 fe80::8a66:3903:23:bcd1/64 scope link valid_lft forever preferred_lft forever 7: ib1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 4092 qdisc pfifo_fast state DOWN group default qlen 256 link/infiniband 80:00:00:29:fe:80:00:00:00:00:00:00:88:66:39:03:00:23:bc:d2 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
2024-01-18 15:01:30,089 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.89.95:38239'
2024-01-18 15:01:30,134 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.89.95:33742'
2024-01-18 15:01:31,503 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/dask-worker-space/worker-zhmndr6g', purging
2024-01-18 15:01:31,503 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/dask-worker-space/worker-o6q6ucyp', purging
2024-01-18 15:01:47,651 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.95:34599
2024-01-18 15:01:47,651 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.95:42610
2024-01-18 15:01:47,652 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.95:34599
2024-01-18 15:01:47,652 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.95:42610
2024-01-18 15:01:47,652 - distributed.worker - INFO -           Worker name:         SLURMCluster-118-0
2024-01-18 15:01:47,652 - distributed.worker - INFO -           Worker name:         SLURMCluster-118-1
2024-01-18 15:01:47,652 - distributed.worker - INFO -          dashboard at:           10.3.89.95:37772
2024-01-18 15:01:47,652 - distributed.worker - INFO -          dashboard at:           10.3.89.95:33884
2024-01-18 15:01:47,652 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,652 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,652 - distributed.worker - INFO -               Threads:                          2
2024-01-18 15:01:47,652 - distributed.worker - INFO -               Threads:                          2
2024-01-18 15:01:47,652 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 15:01:47,652 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 15:01:47,652 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-tcjcd1z1
2024-01-18 15:01:47,652 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-poc2hvjb
2024-01-18 15:01:47,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,730 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,731 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,731 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 15:01:47,731 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,732 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,732 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 15:25:35,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 15:25:35,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 16:02:34,032 - distributed.comm.tcp - INFO - Connection from tcp://10.3.88.12:58630 closed before handshake completed
2024-01-18 16:02:34,033 - distributed.comm.tcp - INFO - Connection from tcp://10.3.88.12:53584 closed before handshake completed
2024-01-18 16:05:09,557 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://10.3.89.95:42610'. Shutting down.
2024-01-18 16:05:09,557 - distributed.worker - INFO - Stopping worker at tcp://10.3.89.95:42610. Reason: worker-close
2024-01-18 16:05:09,558 - distributed.core - INFO - Connection to tcp://10.3.88.12:42763 has been closed.
2024-01-18 16:05:09,560 - distributed.nanny - INFO - Worker closed
2024-01-18 16:05:09,562 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://10.3.89.95:34599'. Shutting down.
2024-01-18 16:05:09,562 - distributed.worker - INFO - Stopping worker at tcp://10.3.89.95:34599. Reason: worker-close
2024-01-18 16:05:09,563 - distributed.core - INFO - Connection to tcp://10.3.88.12:42763 has been closed.
2024-01-18 16:05:09,564 - distributed.nanny - INFO - Worker closed
2024-01-18 16:05:11,561 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-18 16:05:11,566 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-18 16:05:22,123 - distributed.nanny - WARNING - Restarting worker
2024-01-18 16:05:22,221 - distributed.nanny - WARNING - Restarting worker
2024-01-18 16:05:26,079 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.95:44959
2024-01-18 16:05:26,079 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.95:36753
2024-01-18 16:05:26,079 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.95:44959
2024-01-18 16:05:26,079 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.95:36753
2024-01-18 16:05:26,079 - distributed.worker - INFO -           Worker name:         SLURMCluster-118-0
2024-01-18 16:05:26,079 - distributed.worker - INFO -           Worker name:         SLURMCluster-118-1
2024-01-18 16:05:26,079 - distributed.worker - INFO -          dashboard at:           10.3.89.95:39143
2024-01-18 16:05:26,079 - distributed.worker - INFO -          dashboard at:           10.3.89.95:37605
2024-01-18 16:05:26,079 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:05:26,080 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:05:26,080 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:26,080 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:26,080 - distributed.worker - INFO -               Threads:                          2
2024-01-18 16:05:26,080 - distributed.worker - INFO -               Threads:                          2
2024-01-18 16:05:26,080 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 16:05:26,080 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 16:05:26,080 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-5hrp1urv
2024-01-18 16:05:26,080 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-d32kl_cs
2024-01-18 16:05:26,080 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:26,080 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:56,080 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:05:56,080 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:26,182 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:26,182 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:56,284 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:56,284 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:26,385 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:26,386 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:56,487 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:56,488 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:26,588 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:26,590 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:56,689 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:56,691 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:26,791 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:26,793 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:56,892 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:56,894 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:26,995 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:26,997 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:57,097 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:57,100 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:27,199 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:27,201 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:57,301 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:57,302 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:27,403 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:27,403 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:57,505 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:57,505 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:27,608 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:27,608 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:57,709 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:57,709 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:27,811 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:27,811 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:57,913 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:57,913 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:28,015 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:28,015 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:58,117 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:58,117 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:28,218 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:28,218 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:58,321 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:58,321 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:28,423 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:28,423 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:58,524 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:58,524 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:28,625 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:28,625 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:58,726 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:58,727 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:28,828 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:28,828 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:58,931 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:58,931 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:29,032 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:29,033 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:59,134 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:59,134 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:29,235 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:29,236 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:59,336 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:59,337 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:29,437 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:29,439 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:59,539 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:59,541 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:29,640 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:29,642 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:59,742 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:59,743 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:29,843 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:29,844 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:59,946 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:59,946 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:25:26,190 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 16:25:26,190 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:25:26,191 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 16:25:26,192 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 16:25:26,192 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:25:26,192 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 17:00:43,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 140.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 17:00:50,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/__init__.py:55: UserWarning: Dependency not satisfied, torchani.ase will not be available
  warnings.warn("Dependency not satisfied, torchani.ase will not be available")
Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/__init__.py:55: UserWarning: Dependency not satisfied, torchani.ase will not be available
  warnings.warn("Dependency not satisfied, torchani.ase will not be available")
Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.
2024-01-18 17:02:03,816 - distributed.worker - WARNING - Compute Failed
Key:       evaluate-a04aee5f-f36f-40a7-8be2-6fe6e2dcff37
Function:  evaluate
args:      (<rdkit.Chem.rdchem.Mol object at 0x2b82cbc738b0>, 6, '[H]c1nc([H])c(Oc2c([H])nn([H])c2-c2c([H])c([H])c3c([H])c([H])n([H])c3c2[H])c([H])c1[H]', '/mnt/storage/nobackup/nmb1063/code/gal/sars-cov-2-main-protease-al-study/rec_final.pdb', '/nobackup/nmb1063/sars-fegrow/gnina')
kwargs:    {}
Exception: 'RuntimeError("Can\'t redefine method: forward on class: __torch__.torchani.aev.AEVComputer (of Python compilation unit at: 0x2b839fdad610)")'

TIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 7 conformers. 
Removed 6 conformers. 
Generated 8 conformers. 
Removed 7 conformers. 
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                               | 0/1 [00:00<?, ?it/s][W BinaryOps.cpp:601] Warning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (function operator())
2024-01-18 17:02:06,007 - distributed.utils_perf - INFO - full garbage collection released 13.07 MiB from 250980 reference cycles (threshold: 9.54 MiB)
TIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 15 conformers. 
Removed 10 conformers. 
Generated 21 conformers. 
Removed 14 conformers. 
using ani2x
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
failed to equip `nnpops` with error: No module named 'NNPOps'
TIME changed dir: 0.0s
Optimising conformer:   0%|                               | 0/7 [00:00<?, ?it/s][W BinaryOps.cpp:601] Warning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (function operator())
Optimising conformer: 100%|███████████████████████| 1/1 [00:02<00:00,  2.31s/it]Optimising conformer: 100%|███████████████████████| 1/1 [00:02<00:00,  2.31s/it]
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
Optimising conformer:  14%|███▎                   | 1/7 [00:03<00:19,  3.26s/it]Optimising conformer:  29%|██████▌                | 2/7 [00:05<00:14,  2.82s/it]using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
TIME Completed the molecule generation in 81.8s.
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                               | 0/1 [00:00<?, ?it/s]Optimising conformer:  43%|█████████▊             | 3/7 [00:08<00:11,  2.81s/it]Optimising conformer:  57%|█████████████▏         | 4/7 [00:11<00:08,  2.75s/it]Optimising conformer: 100%|███████████████████████| 1/1 [00:03<00:00,  3.25s/it]Optimising conformer: 100%|███████████████████████| 1/1 [00:03<00:00,  3.25s/it]
Optimising conformer:  71%|████████████████▍      | 5/7 [00:24<00:12,  6.38s/it]Optimising conformer:  86%|███████████████████▋   | 6/7 [00:26<00:04,  4.99s/it]Optimising conformer: 100%|███████████████████████| 7/7 [00:28<00:00,  4.13s/it]Optimising conformer: 100%|███████████████████████| 7/7 [00:28<00:00,  4.10s/it]
Generated 14 conformers. 
Removed 8 conformers. 
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
TIME Completed the molecule generation in 118.1s.
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                               | 0/6 [00:00<?, ?it/s]Optimising conformer:  17%|███▊                   | 1/6 [00:01<00:09,  1.90s/it]Optimising conformer:  33%|███████▋               | 2/6 [00:03<00:07,  1.86s/it]Optimising conformer:  50%|███████████▌           | 3/6 [00:09<00:11,  3.78s/it]Optimising conformer:  67%|███████████████▎       | 4/6 [00:10<00:05,  2.74s/it]Optimising conformer:  83%|███████████████████▏   | 5/6 [00:12<00:02,  2.46s/it]Optimising conformer: 100%|███████████████████████| 6/6 [00:13<00:00,  1.95s/it]Optimising conformer: 100%|███████████████████████| 6/6 [00:13<00:00,  2.31s/it]
2024-01-18 17:10:28,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 17:10:28,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[19:52:42] Explicit valence for atom # 7 O, 3, is greater than permitted
[19:52:43] Explicit valence for atom # 7 O, 3, is greater than permitted
[19:52:43] Explicit valence for atom # 7 O, 3, is greater than permitted
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
TIME Completed the molecule generation in 91.4s.
TIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 35 conformers. 
Generated 17 conformers. 
Removed 9 conformers. 
Removed 26 conformers. 
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                               | 0/9 [00:00<?, ?it/s]Optimising conformer:  11%|██▌                    | 1/9 [00:02<00:17,  2.23s/it]Optimising conformer:  22%|█████                  | 2/9 [00:03<00:11,  1.58s/it]Optimising conformer:  33%|███████▋               | 3/9 [00:05<00:10,  1.73s/it]Optimising conformer:  44%|██████████▏            | 4/9 [00:08<00:11,  2.33s/it]Optimising conformer:  56%|████████████▊          | 5/9 [00:10<00:08,  2.22s/it]Optimising conformer:  67%|███████████████▎       | 6/9 [00:12<00:06,  2.06s/it]Optimising conformer:  78%|█████████████████▉     | 7/9 [00:15<00:04,  2.42s/it]Optimising conformer:  89%|████████████████████▍  | 8/9 [00:17<00:02,  2.30s/it]Optimising conformer: 100%|███████████████████████| 9/9 [00:18<00:00,  1.92s/it]Optimising conformer: 100%|███████████████████████| 9/9 [00:18<00:00,  2.06s/it]
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                               | 0/8 [00:00<?, ?it/s]Optimising conformer:  12%|██▉                    | 1/8 [00:02<00:19,  2.79s/it]Optimising conformer:  25%|█████▊                 | 2/8 [00:04<00:12,  2.13s/it]Optimising conformer:  38%|████████▋              | 3/8 [00:06<00:10,  2.03s/it]Optimising conformer:  50%|███████████▌           | 4/8 [00:08<00:07,  1.97s/it]Optimising conformer:  62%|██████████████▍        | 5/8 [00:09<00:05,  1.76s/it]Optimising conformer:  75%|█████████████████▎     | 6/8 [00:11<00:03,  1.78s/it]Optimising conformer:  88%|████████████████████▏  | 7/8 [00:13<00:01,  1.78s/it]Optimising conformer: 100%|███████████████████████| 8/8 [00:15<00:00,  1.78s/it]Optimising conformer: 100%|███████████████████████| 8/8 [00:15<00:00,  1.88s/it]
slurmstepd: error: *** JOB 19786412 ON sb015 CANCELLED AT 2024-01-19T06:01:31 DUE TO TIME LIMIT ***
