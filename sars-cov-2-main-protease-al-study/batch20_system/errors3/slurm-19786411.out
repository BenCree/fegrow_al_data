hostname sb011.cluster
ipconfig 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc mq state UP group default qlen 1000 link/ether a0:8c:f8:61:07:a6 brd ff:ff:ff:ff:ff:ff inet 10.3.81.91/22 brd 10.3.83.255 scope global enp4s0f0 valid_lft forever preferred_lft forever inet6 fe80::a28c:f8ff:fe61:7a6/64 scope link valid_lft forever preferred_lft forever 3: enp4s0f1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:61:07:a7 brd ff:ff:ff:ff:ff:ff 4: enp4s0f2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:61:07:a8 brd ff:ff:ff:ff:ff:ff 5: enp4s0f3: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:61:07:a9 brd ff:ff:ff:ff:ff:ff 6: ib0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 4092 qdisc pfifo_fast state UP group default qlen 256 link/infiniband 80:00:00:29:fe:80:00:00:00:00:00:00:88:66:39:03:00:23:bb:81 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff inet 10.3.89.91/22 brd 10.3.91.255 scope global ib0 valid_lft forever preferred_lft forever inet 10.3.93.91/22 brd 10.3.95.255 scope global ib0:1 valid_lft forever preferred_lft forever inet6 fe80::8a66:3903:23:bb81/64 scope link valid_lft forever preferred_lft forever 7: ib1: <BROADCAST,MULTICAST> mtu 4092 qdisc noop state DOWN group default qlen 256 link/infiniband 80:00:00:29:fe:80:00:00:00:00:00:00:88:66:39:03:00:23:bb:82 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
2024-01-18 15:01:29,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.89.91:46500'
2024-01-18 15:01:29,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.89.91:37298'
2024-01-18 15:01:47,651 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.91:44992
2024-01-18 15:01:47,652 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.91:45929
2024-01-18 15:01:47,652 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.91:44992
2024-01-18 15:01:47,652 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.91:45929
2024-01-18 15:01:47,652 - distributed.worker - INFO -           Worker name:         SLURMCluster-370-1
2024-01-18 15:01:47,652 - distributed.worker - INFO -           Worker name:         SLURMCluster-370-0
2024-01-18 15:01:47,652 - distributed.worker - INFO -          dashboard at:           10.3.89.91:43632
2024-01-18 15:01:47,652 - distributed.worker - INFO -          dashboard at:           10.3.89.91:41070
2024-01-18 15:01:47,652 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,652 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,652 - distributed.worker - INFO -               Threads:                          2
2024-01-18 15:01:47,652 - distributed.worker - INFO -               Threads:                          2
2024-01-18 15:01:47,653 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 15:01:47,653 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 15:01:47,653 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-7n_5t__5
2024-01-18 15:01:47,653 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-du5kzzpb
2024-01-18 15:01:47,653 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,653 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,728 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,728 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,728 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 15:01:47,729 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 15:01:47,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 15:01:47,730 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 15:25:35,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 15:25:35,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 16:02:33,965 - distributed.comm.tcp - INFO - Connection from tcp://10.3.88.12:43522 closed before handshake completed
2024-01-18 16:02:34,063 - distributed.comm.tcp - INFO - Connection from tcp://10.3.88.12:45954 closed before handshake completed
2024-01-18 16:05:09,550 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://10.3.89.91:45929'. Shutting down.
2024-01-18 16:05:09,550 - distributed.worker - INFO - Stopping worker at tcp://10.3.89.91:45929. Reason: worker-close
2024-01-18 16:05:09,550 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://10.3.89.91:44992'. Shutting down.
2024-01-18 16:05:09,551 - distributed.worker - INFO - Stopping worker at tcp://10.3.89.91:44992. Reason: worker-close
2024-01-18 16:05:09,552 - distributed.core - INFO - Connection to tcp://10.3.88.12:42763 has been closed.
2024-01-18 16:05:09,552 - distributed.core - INFO - Connection to tcp://10.3.88.12:42763 has been closed.
2024-01-18 16:05:09,553 - distributed.nanny - INFO - Worker closed
2024-01-18 16:05:09,554 - distributed.nanny - INFO - Worker closed
2024-01-18 16:05:11,555 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-18 16:05:11,555 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-18 16:05:22,097 - distributed.nanny - WARNING - Restarting worker
2024-01-18 16:05:22,098 - distributed.nanny - WARNING - Restarting worker
2024-01-18 16:05:26,086 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.91:41716
2024-01-18 16:05:26,086 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.91:41716
2024-01-18 16:05:26,087 - distributed.worker - INFO -           Worker name:         SLURMCluster-370-1
2024-01-18 16:05:26,087 - distributed.worker - INFO -          dashboard at:           10.3.89.91:39471
2024-01-18 16:05:26,087 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:05:26,087 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:26,087 - distributed.worker - INFO -               Threads:                          2
2024-01-18 16:05:26,087 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 16:05:26,087 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-9v58qvhp
2024-01-18 16:05:26,087 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:26,087 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.91:44402
2024-01-18 16:05:26,088 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.91:44402
2024-01-18 16:05:26,088 - distributed.worker - INFO -           Worker name:         SLURMCluster-370-0
2024-01-18 16:05:26,088 - distributed.worker - INFO -          dashboard at:           10.3.89.91:34257
2024-01-18 16:05:26,088 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:05:26,088 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:26,088 - distributed.worker - INFO -               Threads:                          2
2024-01-18 16:05:26,088 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-18 16:05:26,088 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-w_xcwbf4
2024-01-18 16:05:26,088 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:05:56,088 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:05:56,089 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:26,190 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:26,191 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:56,292 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:06:56,294 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:26,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:26,395 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:56,495 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:07:56,498 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:26,597 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:26,599 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:56,699 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:08:56,700 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:26,801 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:26,801 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:56,903 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:09:56,903 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:27,010 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:27,010 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:57,111 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:10:57,112 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:27,213 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:27,213 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:57,314 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:11:57,315 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:27,416 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:27,416 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:57,518 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:12:57,518 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:27,619 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:27,620 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:57,722 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:13:57,722 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:27,824 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:27,825 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:57,926 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:14:57,928 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:28,034 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:28,034 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:58,136 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:15:58,136 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:28,241 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:28,242 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:58,342 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:16:58,343 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:28,444 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:28,444 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:58,547 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:17:58,547 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:28,648 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:28,650 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:58,750 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:18:58,751 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:28,851 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:28,852 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:58,953 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:19:58,953 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:29,056 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:29,056 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:59,157 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:20:59,159 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:29,259 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:29,260 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:59,361 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:21:59,362 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:29,463 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:29,463 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:59,566 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:22:59,566 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:29,667 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:29,668 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:59,769 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:23:59,770 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:29,871 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:29,872 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:59,973 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:24:59,974 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-18 16:25:26,194 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 16:25:26,196 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:25:26,197 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-18 16:25:26,197 - distributed.worker - INFO - -------------------------------------------------
2024-01-18 16:25:26,197 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 16:25:26,198 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-18 17:00:43,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 140.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 17:00:50,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/__init__.py:55: UserWarning: Dependency not satisfied, torchani.ase will not be available
  warnings.warn("Dependency not satisfied, torchani.ase will not be available")
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/__init__.py:55: UserWarning: Dependency not satisfied, torchani.ase will not be available
  warnings.warn("Dependency not satisfied, torchani.ase will not be available")
Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.
Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.
2024-01-18 17:02:03,918 - distributed.worker - WARNING - Compute Failed
Key:       evaluate-2463997e-8f4f-46a4-b0a9-d2f719e9b3bc
Function:  evaluate
args:      (<rdkit.Chem.rdchem.Mol object at 0x2b64219e14a0>, 6, '[H]c1nc([H])c(-c2c([H])c([H])c(=O)n(-c3c([H])c([H])c([H])c(F)c3[H])c2[H])c([H])c1[H]', '/mnt/storage/nobackup/nmb1063/code/gal/sars-cov-2-main-protease-al-study/rec_final.pdb', '/nobackup/nmb1063/sars-fegrow/gnina')
kwargs:    {}
Exception: 'RuntimeError("Can\'t redefine method: forward on class: __torch__.torchani.aev.AEVComputer (of Python compilation unit at: 0x2b6425f90700)")'

TIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 16 conformers. 
Removed 2 conformers. 
Generated 33 conformers. 
Removed 14 conformers. 
using ani2x
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                              | 0/19 [00:00<?, ?it/s][W BinaryOps.cpp:601] Warning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (function operator())
TIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 24 conformers. 
Generated 14 conformers. 
Removed 19 conformers. 
Removed 12 conformers. 
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                               | 0/5 [00:00<?, ?it/s][W BinaryOps.cpp:601] Warning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (function operator())
Optimising conformer:   5%|█▏                    | 1/19 [00:02<00:53,  2.96s/it]Optimising conformer:  11%|██▎                   | 2/19 [00:05<00:46,  2.76s/it]Optimising conformer:  20%|████▌                  | 1/5 [00:03<00:14,  3.64s/it]failed to equip `nnpops` with error: No module named 'NNPOps'

Optimising conformer:   0%|                               | 0/2 [00:00<?, ?it/s][AOptimising conformer:  40%|█████████▏             | 2/5 [00:05<00:07,  2.64s/it]Optimising conformer:  16%|███▍                  | 3/19 [00:09<00:51,  3.22s/it]
Optimising conformer:  50%|███████████▌           | 1/2 [00:04<00:04,  4.22s/it][AOptimising conformer:  21%|████▋                 | 4/19 [00:11<00:40,  2.70s/it]Optimising conformer:  60%|█████████████▊         | 3/5 [00:09<00:06,  3.14s/it]Optimising conformer:  80%|██████████████████▍    | 4/5 [00:10<00:02,  2.55s/it]Optimising conformer:  26%|█████▊                | 5/19 [00:13<00:34,  2.45s/it]Optimising conformer:  32%|██████▉               | 6/19 [00:15<00:30,  2.32s/it]
Optimising conformer: 100%|███████████████████████| 2/2 [00:10<00:00,  5.62s/it][AOptimising conformer: 100%|███████████████████████| 2/2 [00:10<00:00,  5.41s/it]
Optimising conformer:  37%|████████              | 7/19 [00:18<00:32,  2.74s/it]Optimising conformer: 100%|███████████████████████| 5/5 [00:17<00:00,  3.84s/it]Optimising conformer: 100%|███████████████████████| 5/5 [00:17<00:00,  3.42s/it]
Optimising conformer:  42%|█████████▎            | 8/19 [00:21<00:29,  2.70s/it]Optimising conformer:  47%|██████████▍           | 9/19 [00:23<00:24,  2.41s/it]Optimising conformer:  53%|███████████          | 10/19 [00:26<00:23,  2.65s/it]Optimising conformer:  58%|████████████▏        | 11/19 [00:28<00:20,  2.52s/it]Optimising conformer:  63%|█████████████▎       | 12/19 [00:30<00:15,  2.24s/it]Optimising conformer:  68%|██████████████▎      | 13/19 [00:33<00:15,  2.52s/it]Optimising conformer:  74%|███████████████▍     | 14/19 [00:35<00:11,  2.24s/it]Optimising conformer:  79%|████████████████▌    | 15/19 [00:36<00:08,  2.05s/it]Optimising conformer:  84%|█████████████████▋   | 16/19 [00:38<00:05,  1.93s/it]Optimising conformer:  89%|██████████████████▊  | 17/19 [00:41<00:04,  2.26s/it]Optimising conformer:  95%|███████████████████▉ | 18/19 [00:43<00:02,  2.07s/it]Optimising conformer: 100%|█████████████████████| 19/19 [00:45<00:00,  2.34s/it]Optimising conformer: 100%|█████████████████████| 19/19 [00:45<00:00,  2.42s/it]
2024-01-18 17:13:36,842 - distributed.utils_perf - INFO - full garbage collection released 9.99 MiB from 481279 reference cycles (threshold: 9.54 MiB)
2024-01-18 17:16:02,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 17:16:23,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-18 17:16:23,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
TIME Completed the molecule generation in 103.6s.
TIME Completed the molecule generation in 105.6s.
TIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 15 conformers. 
Removed 8 conformers. 
Generated 32 conformers. 
Removed 23 conformers. 
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
Optimising conformer:   0%|                               | 0/7 [00:00<?, ?it/s]Optimising conformer:  14%|███▎                   | 1/7 [00:03<00:20,  3.39s/it]failed to equip `nnpops` with error: No module named 'NNPOps'

Optimising conformer:   0%|                               | 0/9 [00:00<?, ?it/s][AOptimising conformer:  29%|██████▌                | 2/7 [00:05<00:13,  2.75s/it]Optimising conformer:  43%|█████████▊             | 3/7 [00:08<00:10,  2.59s/it]
Optimising conformer:  11%|██▌                    | 1/9 [00:03<00:29,  3.71s/it][AOptimising conformer:  57%|█████████████▏         | 4/7 [00:10<00:07,  2.34s/it]
Optimising conformer:  22%|█████                  | 2/9 [00:05<00:18,  2.60s/it][A
Optimising conformer:  33%|███████▋               | 3/9 [00:07<00:14,  2.45s/it][AOptimising conformer:  71%|████████████████▍      | 5/7 [00:12<00:05,  2.52s/it]
Optimising conformer:  44%|██████████▏            | 4/9 [00:09<00:10,  2.17s/it][AOptimising conformer:  86%|███████████████████▋   | 6/7 [00:14<00:02,  2.33s/it]
Optimising conformer:  56%|████████████▊          | 5/9 [00:11<00:07,  1.93s/it][AOptimising conformer: 100%|███████████████████████| 7/7 [00:16<00:00,  2.06s/it]Optimising conformer: 100%|███████████████████████| 7/7 [00:16<00:00,  2.34s/it]

Optimising conformer:  67%|███████████████▎       | 6/9 [00:12<00:05,  1.85s/it][A
Optimising conformer:  78%|█████████████████▉     | 7/9 [00:14<00:03,  1.78s/it][A
Optimising conformer:  89%|████████████████████▍  | 8/9 [00:16<00:01,  1.86s/it][A
Optimising conformer: 100%|███████████████████████| 9/9 [00:18<00:00,  1.88s/it][AOptimising conformer: 100%|███████████████████████| 9/9 [00:18<00:00,  2.04s/it]
2024-01-18 18:16:11,205 - distributed.utils_perf - INFO - full garbage collection released 27.48 MiB from 475996 reference cycles (threshold: 9.54 MiB)
slurmstepd: error: *** JOB 19786411 ON sb011 CANCELLED AT 2024-01-19T06:01:31 DUE TO TIME LIMIT ***
