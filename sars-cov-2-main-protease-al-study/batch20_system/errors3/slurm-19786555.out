hostname sb012.cluster
ipconfig 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc mq state UP group default qlen 1000 link/ether a0:8c:f8:b9:a3:22 brd ff:ff:ff:ff:ff:ff inet 10.3.81.92/22 brd 10.3.83.255 scope global enp4s0f0 valid_lft forever preferred_lft forever inet6 fe80::a28c:f8ff:feb9:a322/64 scope link valid_lft forever preferred_lft forever 3: enp4s0f1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:b9:a3:23 brd ff:ff:ff:ff:ff:ff 4: enp4s0f2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:b9:a3:24 brd ff:ff:ff:ff:ff:ff 5: enp4s0f3: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether a0:8c:f8:b9:a3:25 brd ff:ff:ff:ff:ff:ff 6: ib0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 4092 qdisc pfifo_fast state UP group default qlen 256 link/infiniband 80:00:00:29:fe:80:00:00:00:00:00:00:88:66:39:03:00:23:ba:71 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff inet 10.3.89.92/22 brd 10.3.91.255 scope global ib0 valid_lft forever preferred_lft forever inet 10.3.97.92/22 brd 10.3.99.255 scope global ib0:1 valid_lft forever preferred_lft forever inet6 fe80::8a66:3903:23:ba71/64 scope link valid_lft forever preferred_lft forever 7: ib1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 4092 qdisc pfifo_fast state DOWN group default qlen 256 link/infiniband 80:00:00:29:fe:80:00:00:00:00:00:00:88:66:39:03:00:23:ba:72 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
2024-01-19 07:55:51,743 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.89.92:35151'
2024-01-19 07:55:51,757 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.89.92:42245'
2024-01-19 07:55:55,256 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.92:34252
2024-01-19 07:55:55,256 - distributed.worker - INFO -       Start worker at:     tcp://10.3.89.92:37524
2024-01-19 07:55:55,257 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.92:34252
2024-01-19 07:55:55,257 - distributed.worker - INFO -          Listening to:     tcp://10.3.89.92:37524
2024-01-19 07:55:55,257 - distributed.worker - INFO -           Worker name:         SLURMCluster-418-1
2024-01-19 07:55:55,257 - distributed.worker - INFO -           Worker name:         SLURMCluster-418-0
2024-01-19 07:55:55,257 - distributed.worker - INFO -          dashboard at:           10.3.89.92:44728
2024-01-19 07:55:55,257 - distributed.worker - INFO -          dashboard at:           10.3.89.92:33494
2024-01-19 07:55:55,257 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-19 07:55:55,257 - distributed.worker - INFO - Waiting to connect to:     tcp://10.3.88.12:42763
2024-01-19 07:55:55,257 - distributed.worker - INFO - -------------------------------------------------
2024-01-19 07:55:55,257 - distributed.worker - INFO - -------------------------------------------------
2024-01-19 07:55:55,257 - distributed.worker - INFO -               Threads:                          2
2024-01-19 07:55:55,257 - distributed.worker - INFO -               Threads:                          2
2024-01-19 07:55:55,258 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-19 07:55:55,258 - distributed.worker - INFO -                Memory:                  10.24 GiB
2024-01-19 07:55:55,258 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-5sgjxorm
2024-01-19 07:55:55,258 - distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-wc_p9s_q
2024-01-19 07:55:55,258 - distributed.worker - INFO - -------------------------------------------------
2024-01-19 07:55:55,258 - distributed.worker - INFO - -------------------------------------------------
2024-01-19 07:55:55,289 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-19 07:55:55,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-19 07:55:55,290 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-19 07:55:55,293 - distributed.worker - INFO -         Registered to:     tcp://10.3.88.12:42763
2024-01-19 07:55:55,293 - distributed.worker - INFO - -------------------------------------------------
2024-01-19 07:55:55,294 - distributed.core - INFO - Starting established connection to tcp://10.3.88.12:42763
2024-01-19 08:04:39,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-01-19 08:04:39,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/parmed/structure.py:1775: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.
  coords = np.array(value, dtype=np.float64, copy=False, subok=True)
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/__init__.py:55: UserWarning: Dependency not satisfied, torchani.ase will not be available
  warnings.warn("Dependency not satisfied, torchani.ase will not be available")
Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.
TIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 39 conformers. 
Generated 38 conformers. 
Removed 35 conformers. 
Removed 26 conformers. 
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                              | 0/12 [00:00<?, ?it/s][W BinaryOps.cpp:601] Warning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (function operator())
Optimising conformer:   8%|█▊                    | 1/12 [00:02<00:32,  2.94s/it]/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/__init__.py:55: UserWarning: Dependency not satisfied, torchani.ase will not be available
  warnings.warn("Dependency not satisfied, torchani.ase will not be available")
Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.
Optimising conformer:  17%|███▋                  | 2/12 [00:07<00:37,  3.74s/it]Optimising conformer:  25%|█████▌                | 3/12 [00:09<00:29,  3.24s/it]using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'

Optimising conformer:   0%|                               | 0/4 [00:00<?, ?it/s][ATIME changed dir: 0.0s
TIME changed dir: 0.0s
Generated 16 conformers. 
Removed 10 conformers. 
Generated 20 conformers. 
Removed 10 conformers. 
using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'
Optimising conformer:   0%|                              | 0/10 [00:00<?, ?it/s][W BinaryOps.cpp:601] Warning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (function operator())
Optimising conformer:  33%|███████▎              | 4/12 [00:12<00:24,  3.12s/it]
Optimising conformer:  25%|█████▊                 | 1/4 [00:03<00:09,  3.11s/it][AOptimising conformer:  42%|█████████▏            | 5/12 [00:15<00:19,  2.80s/it]
Optimising conformer:  50%|███████████▌           | 2/4 [00:05<00:04,  2.43s/it][A
Optimising conformer:  75%|█████████████████▎     | 3/4 [00:06<00:02,  2.19s/it][AOptimising conformer:  10%|██▏                   | 1/10 [00:06<00:54,  6.00s/it]
Optimising conformer: 100%|███████████████████████| 4/4 [00:08<00:00,  2.11s/it][AOptimising conformer: 100%|███████████████████████| 4/4 [00:08<00:00,  2.24s/it]
Optimising conformer:  50%|███████████           | 6/12 [00:19<00:20,  3.37s/it]Optimising conformer:  20%|████▍                 | 2/10 [00:08<00:31,  3.90s/it]Optimising conformer:  58%|████████████▊         | 7/12 [00:22<00:16,  3.31s/it]Optimising conformer:  30%|██████▌               | 3/10 [00:11<00:23,  3.39s/it]Optimising conformer:  67%|██████████████▋       | 8/12 [00:24<00:11,  2.88s/it]Optimising conformer:  40%|████████▊             | 4/10 [00:13<00:16,  2.76s/it]using ani2x
/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/torchani/resources/
failed to equip `nnpops` with error: No module named 'NNPOps'

Optimising conformer:   0%|                               | 0/6 [00:00<?, ?it/s][AOptimising conformer:  50%|███████████           | 5/10 [00:14<00:11,  2.33s/it]Optimising conformer:  60%|█████████████▏        | 6/10 [00:17<00:09,  2.38s/it]
Optimising conformer:  17%|███▊                   | 1/6 [00:04<00:21,  4.20s/it][A
Optimising conformer:  33%|███████▋               | 2/6 [00:06<00:11,  2.88s/it][A
Optimising conformer:  50%|███████████▌           | 3/6 [00:09<00:08,  3.00s/it][A
Optimising conformer:  67%|███████████████▎       | 4/6 [00:12<00:06,  3.12s/it][A
Optimising conformer:  83%|███████████████████▏   | 5/6 [00:14<00:02,  2.77s/it][AOptimising conformer:  75%|████████████████▌     | 9/12 [00:42<00:22,  7.56s/it]
Optimising conformer: 100%|███████████████████████| 6/6 [00:17<00:00,  2.64s/it][AOptimising conformer: 100%|███████████████████████| 6/6 [00:17<00:00,  2.86s/it]
Optimising conformer:  83%|█████████████████▌   | 10/12 [00:44<00:11,  5.76s/it]Optimising conformer:  92%|███████████████████▎ | 11/12 [00:49<00:05,  5.64s/it]Optimising conformer: 100%|█████████████████████| 12/12 [00:52<00:00,  4.83s/it]Optimising conformer: 100%|█████████████████████| 12/12 [00:52<00:00,  4.38s/it]
Optimising conformer:  70%|███████████████▍      | 7/10 [00:42<00:30, 10.07s/it]Optimising conformer:  80%|█████████████████▌    | 8/10 [00:44<00:14,  7.31s/it]Optimising conformer:  90%|███████████████████▊  | 9/10 [01:15<00:14, 14.72s/it]Optimising conformer: 100%|█████████████████████| 10/10 [01:16<00:00, 10.64s/it]Optimising conformer: 100%|█████████████████████| 10/10 [01:16<00:00,  7.69s/it]
2024-01-19 09:24:31,900 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/worker.py", line 1237, in heartbeat
    response = await retry_operation(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1227, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.3.89.92:40508 remote=tcp://10.3.88.12:42763>: TimeoutError: [Errno 110] Connection timed out
2024-01-19 09:24:31,900 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/worker.py", line 1237, in heartbeat
    response = await retry_operation(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1227, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.3.89.92:40506 remote=tcp://10.3.88.12:42763>: TimeoutError: [Errno 110] Connection timed out
2024-01-19 09:24:46,876 - distributed.core - INFO - Connection to tcp://10.3.88.12:42763 has been closed.
2024-01-19 09:24:46,876 - distributed.core - INFO - Connection to tcp://10.3.88.12:42763 has been closed.
2024-01-19 09:24:46,879 - distributed.worker - INFO - Stopping worker at tcp://10.3.89.92:37524. Reason: worker-handle-scheduler-connection-broken
2024-01-19 09:24:46,879 - distributed.worker - INFO - Stopping worker at tcp://10.3.89.92:34252. Reason: worker-handle-scheduler-connection-broken
2024-01-19 09:24:46,888 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.3.89.92:35151'. Reason: worker-handle-scheduler-connection-broken
2024-01-19 09:24:46,889 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.3.89.92:42245'. Reason: worker-handle-scheduler-connection-broken
2024-01-19 09:24:46,904 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1389, in _connect
    comm = await connect(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/comm/core.py", line 291, in connect
    comm = await asyncio.wait_for(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/worker.py", line 1237, in heartbeat
    response = await retry_operation(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1224, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1468, in connect
    return await connect_attempt
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1412, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-19 09:24:46,924 - distributed.nanny - INFO - Worker closed
2024-01-19 09:24:46,904 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1389, in _connect
    comm = await connect(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/comm/core.py", line 291, in connect
    comm = await asyncio.wait_for(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/worker.py", line 1237, in heartbeat
    response = await retry_operation(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1224, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1468, in connect
    return await connect_attempt
  File "/mnt/storage/nobackup/nmb1063/mamba/envs/fegrow/lib/python3.9/site-packages/distributed/core.py", line 1412, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-19 09:24:46,930 - distributed.nanny - INFO - Worker closed
TIME Completed the molecule generation in 79.6s.
TIME Completed the molecule generation in 105.8s.
TIME Completed the molecule generation in 100.1s.
TIME Completed the molecule generation in 138.6s.
2024-01-19 09:24:48,936 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-19 09:24:51,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.3.89.92:42245'. Reason: nanny-close-gracefully
2024-01-19 09:24:51,905 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.3.89.92:35151'. Reason: nanny-close-gracefully
2024-01-19 09:24:51,907 - distributed.dask_worker - INFO - End worker
